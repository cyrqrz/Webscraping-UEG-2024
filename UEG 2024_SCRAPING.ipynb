```json
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "executionInfo": {
          "elapsed": 1213,
          "status": "ok",
          "timestamp": 1728939976104,
          "user": {
            "displayName": "Cyro Queiroz",
            "userId": "10205326658499773298"
          },
          "user_tz": 180
        },
        "id": "WNnSViHy-Qnz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import re\n",
        "import html\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "executionInfo": {
          "elapsed": 483,
          "status": "ok",
          "timestamp": 1728939976582,
          "user": {
            "displayName": "Cyro Queiroz",
            "userId": "10205326658499773298"
          },
          "user_tz": 180
        },
        "id": "JCKVXIx0-Td4"
      },
      "outputs": [],
      "source": [
        "def find_matching_keywords(text, keywords):\n",
        "    if not text:\n",
        "        return None\n",
        "    matches = [keyword for keyword in keywords if keyword and keyword.lower() in text.lower()]\n",
        "    return '; '.join(matches) if matches else None\n",
        "\n",
        "def clean_text(bad_text):\n",
        "    if bad_text is None:  # Check for None\n",
        "        return ''\n",
        "    # 1. Decode HTML entities (e.g., &amp;, &lt;, etc.)\n",
        "    decoded_text = html.unescape(bad_text)\n",
        "    \n",
        "    no_html = re.sub(r'', '', decoded_text)\n",
        "    # 2. Remove HTML tags using regex\n",
        "    clean_text = re.sub(r'[\\x02]', '', no_html)\n",
        "    \n",
        "    # 3. Strip any excessive whitespace\n",
        "    clean_text = clean_text.strip()\n",
        "    \n",
        "    return clean_text\n",
        "\n",
        "def clean_dict(data_dict):\n",
        "    cleaned_data = {}\n",
        "    for key, value in data_dict.items():\n",
        "        if isinstance(value, str):  # Only clean if it's a string\n",
        "            cleaned_data[key] = clean_text(value)\n",
        "        else:\n",
        "            cleaned_data[key] = value  # Leave other types (lists, None, etc.) unchanged\n",
        "    return cleaned_data\n",
        "\n",
        "day_id = 2\n",
        "types = ['week', 'industry']\n",
        "industry_ids = []\n",
        "weeks_ids = []\n",
        "\n",
        "\n",
        "\n",
        "# Read keywords from a local file\n",
        "with open('keywords.json', 'r') as f:\n",
        "    keywords_data = json.load(f)\n",
        "\n",
        "# Keyword lists\n",
        "indication = [row[0] for row in keywords_data['rows']]\n",
        "asset = [row[1] for row in keywords_data['rows']]\n",
        "company_name = [row[2] for row in keywords_data['rows']]\n",
        "MoA = [row[3] for row in keywords_data['rows']]\n",
        "other_keywords = [row[4] for row in keywords_data['rows']]\n",
        "\n",
        "\n",
        "# Fields to extract\n",
        "fields = [\n",
        "    \"Session Date\", \"Session Start Time\", \"Session End Time\", \"Session Timezone\",\n",
        "    \"Session Location\", \"Session Number\", \"Session Pathways\", \"Session Format\",\n",
        "    \"Session CME\", \"Session Type\", \"Session Group\", \"Session Title\", \"Session Description\",\n",
        "    \"Presentation Date\", \"Presentation Location\", \"Presentation Start Time\", \"Presentation End Time\",\n",
        "    \"Presentation ID\", \"Presentation Number\", \"Presentation Title\", \"Abstract Authors\", \n",
        "    \"Abstract Affiliations\", \"Presentation Title (lowercase)\", \"Abstract\", \n",
        "    \"Abstract Url\", \"Company\", \"Asset\", \"Indication\", \"MoA\", \"Other Keywords\"\n",
        "]\n",
        "\n",
        "combined_data = []\n",
        "indication = []\n",
        "asset = []\n",
        "company_name = []\n",
        "MoA = []\n",
        "other_keywords = []\n",
        "allEndpoints = []\n",
        "\n",
        "\n",
        "for row in keywords_data['rows']:\n",
        "    indication.append(row[0])\n",
        "    asset.append(row[1])\n",
        "    company_name.append(row[2])\n",
        "    MoA.append(row[3])\n",
        "    other_keywords.append(row[4])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Iterate over types\n",
        "for event_type in types:\n",
        "    weekFinished = False\n",
        "    indFinished = False\n",
        "    day_id = 2\n",
        "    while not (weekFinished and indFinished):\n",
        "        # Construct the URL for a specific day\n",
        "        days_url = f\"{url}\"\n",
        "        \n",
        "        # Fetch data for the day\n",
        "        response = requests.get(days_url)\n",
        "        day_data = response.json()\n",
        "\n",
        "        # Stop if no data is returned\n",
        "        if not day_data:\n",
        "            if event_type == 'week':\n",
        "                weekFinished = True\n",
        "            elif event_type == 'industry':\n",
        "                indFinished = True\n",
        "            break\n",
        "\n",
        "        for content in day_data:\n",
        "            if event_type == 'industry':\n",
        "                idInd = content.get('id')\n",
        "                industry_ids.append(idInd)\n",
        "            if event_type == 'week':\n",
        "                idWeek = content.get('id')\n",
        "                weeks_ids.append(idWeek)\n",
        "            \n",
        "        day_id += 1\n",
        "\n",
        "\n",
        "all_ids = [\n",
        "    {\"ids\": industry_ids, \"event_type\": \"industry\"},\n",
        "    {\"ids\": weeks_ids, \"event_type\": \"week\"}\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": ""
        }
      ],
      "source": []
    }
  ]
}
```

---
Resposta do Perplexity: pplx.ai/share
